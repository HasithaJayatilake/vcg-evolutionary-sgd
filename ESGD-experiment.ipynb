{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESGD-experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasithaJayatilake/vcg-evolutionary-sgd/blob/experiments/ESGD-experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNFyHkaiEu8h",
        "outputId": "18884aa7-c6b8-40a9-d707-170fdf19a473"
      },
      "source": [
        "!pip install neat-python"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neat-python in /usr/local/lib/python3.7/dist-packages (0.92)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ09mSUFrS_M",
        "outputId": "6db59d0a-12a0-47dd-928c-7e9c84cf9aa4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMl1OrFJ4g4"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys \n",
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S9pJuH_rgja"
      },
      "source": [
        "import sys\n",
        "import neat\n",
        "sys.path.insert(0,'/content/drive/My Drive/2. Adelaide Uni/Research Project/Evolutionary Models/Imports')\n",
        "\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3856G1R_Me8",
        "outputId": "7012c84e-1aea-4cf2-af08-d061733c9707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "from UpperBoundCalculator import UpperBoundCalculator\n",
        "from StatisticsReporter import StatisticsReporter\n",
        "from Reporting import StdOutReporter\n",
        "from Reporting import ReporterSet\n",
        "from Reporting import BaseReporter\n",
        "from Genome import DefaultGenome"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-446a5c615af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mReporting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReporterSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mReporting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseReporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mGenome\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDefaultGenome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Genome'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iavl692t495G"
      },
      "source": [
        "# Uploading the saved model state\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def create_mount_pydrive():\n",
        "    # 1. Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    u_drive = GoogleDrive(gauth)\n",
        "    return u_drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgrDyt55lMqR"
      },
      "source": [
        "upload_drive = create_mount_pydrive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIxP3-x5c7Jd"
      },
      "source": [
        "ONE = torch.tensor(1)\n",
        "ZERO = torch.tensor(0)\n",
        "INF = sys.maxsize\n",
        "n_agents = 4\n",
        "# Simulated data for n agents \n",
        "agent_bid_data = torch.FloatTensor(15000,n_agents).uniform_(0, 1)\n",
        "test_bid_data = torch.FloatTensor(10000,n_agents).uniform_(0, 1)\n",
        "agent_bid_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWJJ45KudDIH"
      },
      "source": [
        "def input_formatter(data):\n",
        "  # Input data has to be in the form of pairs of other peoples bids (for each person)\n",
        "  data_dim = n_agents*(n_agents-1)\n",
        "  formatted_data = torch.zeros(data_dim)\n",
        "\n",
        "  for i, bid_set in enumerate(data):\n",
        "    input_vector = torch.tensor([])\n",
        "    for j, bid in enumerate(bid_set):\n",
        "      input_vector = torch.cat((input_vector,bid_set[:j], bid_set[j+1:]), 0)\n",
        "    # print(input_vector.shape)\n",
        "    formatted_data = torch.vstack((formatted_data,input_vector))\n",
        "\n",
        "  return formatted_data[1:]\n",
        "\n",
        "trainset_sgd = torch.utils.data.DataLoader(agent_bid_data, batch_size=100, shuffle=True)\n",
        "input_data = input_formatter(agent_bid_data)\n",
        "test_data = input_formatter(test_bid_data)\n",
        "input_data_np = input_data.detach().cpu().numpy()\n",
        "test_data_np = test_data.detach().cpu().numpy()\n",
        "\n",
        "uc = UpperBoundCalculator(n_agents)\n",
        "upper_bound = uc.get_upper_bound()\n",
        "print(f'Conjectured upper bound worst-case alpha for {n_agents} agents: {upper_bound}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvhl0sUNryXy"
      },
      "source": [
        "def vcg_constraints (values, rebates, verbose=False):\n",
        "    num_agents = values.shape[0]    \n",
        "    best_utility = max(values.sum(),1)\n",
        "    project_utility = (num_agents)*best_utility\n",
        "    achieved_utility = project_utility-rebates.sum()\n",
        "    efficiency_ratio = achieved_utility/best_utility\n",
        "    efficiency_constraint = rebates.sum() - ((num_agents - efficiency_ratio)*best_utility)\n",
        "    budget_constraint = ((num_agents-1)*best_utility)-rebates.sum()\n",
        " \n",
        "    if verbose:\n",
        "      print('num_agents: ', num_agents)\n",
        "      print('sum of bids: ', values.sum())\n",
        "      print('sum of rebates: ', rebates.sum())\n",
        "      print('best_utility: ', best_utility)\n",
        "      print('project_utility: ', project_utility)\n",
        "      print('achieved_utility: ', achieved_utility)\n",
        "      print('efficiency_ratio: ', efficiency_ratio)\n",
        "      print('budget_constraint: ', budget_constraint)\n",
        " \n",
        "    return budget_constraint, efficiency_constraint, efficiency_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuloCZ7Lz9FY"
      },
      "source": [
        "def eval_single_genome(net, marker):\n",
        "  max_budget_constraint = -INF\n",
        "  max_efficiency_constraint = -INF\n",
        "  worst_case_efficiency = INF\n",
        "  for project in input_data_np:\n",
        "    X = project\n",
        "    output = net.activate(X[:marker])\n",
        "    for i in range(marker):\n",
        "      start = marker*(i+1)\n",
        "      end = marker*(i+2)\n",
        "      output_ = net.activate(X[start:end])\n",
        "      output = np.concatenate((output, output_))\n",
        "    budget_constraint, efficiency_constraint, efficiency_ratio = vcg_constraints(X[:n_agents], output)\n",
        "    max_budget_constraint = max(max_budget_constraint, budget_constraint)\n",
        "    max_efficiency_constraint = max(max_efficiency_constraint, efficiency_constraint)\n",
        "    worst_case_efficiency =  min(worst_case_efficiency, efficiency_ratio)\n",
        "\n",
        "  return max_budget_constraint, max_efficiency_constraint, worst_case_efficiency\n",
        "\n",
        "def wc_efficiency_reward_function(wc_efficiency, ub):\n",
        "    if wc_efficiency<0:\n",
        "        return wc_efficiency*10\n",
        "    else:\n",
        "        return min(wc_efficiency, ub)\n",
        "\n",
        "def eval_genomes_vcg(genomes, config):\n",
        "    marker = n_agents-1\n",
        "    budget_surplus_allowance = 0.1\n",
        "    efficiency_surplus_allowance = 0.1\n",
        "    constraint_surplus = budget_surplus_allowance + budget_surplus_allowance\n",
        "    best_wc_budget_constraint = INF\n",
        "    best_wc_efficiency_constraint = INF\n",
        "    best_wc_efficiency = -INF\n",
        "    best_fitness = -INF\n",
        "    best_genome=None\n",
        "\n",
        "    for genome_id, genome in genomes:\n",
        "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
        "        max_budget_constraint, max_efficiency_constraint, worst_case_efficiency = eval_single_genome(net, marker)\n",
        "        penalty1 = 100*max_budget_constraint if max_budget_constraint>0 else (1e-2)*max_budget_constraint\n",
        "        penalty2 = 10*max_efficiency_constraint if max_efficiency_constraint>0 else 0  \n",
        "        genome.fitness = wc_efficiency_reward_function(worst_case_efficiency, upper_bound) - penalty1 - penalty2\n",
        "        if genome.fitness>best_fitness:\n",
        "            best_fitness = genome.fitness\n",
        "            best_genome = genome\n",
        "            best_wc_budget_constraint = max_budget_constraint\n",
        "            best_wc_efficiency_constraint = max_efficiency_constraint\n",
        "            best_wc_efficiency = worst_case_efficiency\n",
        "    \n",
        "    print('Best budget constraint: {0:3.5f}'.format(best_wc_budget_constraint))    \n",
        "    print('Best efficiency constraint: {0:3.5f}'.format(best_wc_efficiency_constraint))    \n",
        "    print('Best worst-case efficiency: {0:3.5f}'.format(best_wc_efficiency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhogmAUsCiun"
      },
      "source": [
        "from neat.graphs import feed_forward_layers\n",
        "from neat.six_util import itervalues\n",
        "\n",
        "class FeedForwardTorchNet(nn.Module):\n",
        "    \"\"\" Custom Non-Linear layer with some nodes with max aggregation \"\"\"\n",
        "    def __init__(self, inputs, outputs, node_evals):\n",
        "        data_dim = n_agents*(n_agents-1)\n",
        "        formatted_data = torch.zeros(data_dim)\n",
        "\n",
        "        self.input_nodes = inputs\n",
        "        self.output_nodes = outputs\n",
        "        \n",
        "        self.values = dict((key, 0.0) for key in inputs + outputs)\n",
        "        self.node_evals = []\n",
        "        weights_list = []\n",
        "\n",
        "        # Initializing node descriptions with trainable parameters\n",
        "        for node, act_func, agg_func, bias, response, links in node_evals:\n",
        "            bias_param = nn.Parameter(torch.tensor(bias), requires_grad=True)\n",
        "            weight_params_dict = [(i,nn.Parameter(torch.tensor(w), requires_grad=True)) for i, w in links]            \n",
        "            self.node_evals.append((node, act_func, agg_func, bias_param, weight_params_dict))\n",
        "    \n",
        "    @staticmethod\n",
        "    def create(genome, config):\n",
        "        \"\"\" Receives a genome and returns its phenotype (a FeedForwardNetwork). \"\"\"\n",
        "\n",
        "        # Gather expressed connections.\n",
        "        connections = [cg.key for cg in itervalues(genome.connections) if cg.enabled]\n",
        "\n",
        "        layers = feed_forward_layers(config.genome_config.input_keys, config.genome_config.output_keys, connections)\n",
        "        node_evals = []\n",
        "        for layer in layers:\n",
        "            for node in layer:\n",
        "                inputs = []\n",
        "                node_expr = [] # currently unused\n",
        "                for conn_key in connections:\n",
        "                    inode, onode = conn_key\n",
        "                    if onode == node:\n",
        "                        cg = genome.connections[conn_key]\n",
        "                        inputs.append((inode, cg.weight))\n",
        "                        node_expr.append(\"v[{}] * {:.7e}\".format(inode, cg.weight))\n",
        "\n",
        "                ng = genome.nodes[node]\n",
        "                aggregation_function = config.genome_config.aggregation_function_defs.get(ng.aggregation)\n",
        "                activation_function = config.genome_config.activation_defs.get(ng.activation)\n",
        "                node_evals.append((node, activation_function, aggregation_function, ng.bias, ng.response, inputs))\n",
        "\n",
        "        return FeedForwardTorchNet(config.genome_config.input_keys, config.genome_config.output_keys, node_evals)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        if len(self.input_nodes) != len(inputs):\n",
        "            raise RuntimeError(\"Expected {0:n} inputs, got {1:n}\".format(len(self.input_nodes), len(inputs)))\n",
        "\n",
        "        for k, v in zip(self.input_nodes, inputs):\n",
        "            self.values[k] = v\n",
        "\n",
        "        for node, act_func, agg_func, bias, links in self.node_evals:\n",
        "            node_inputs = []\n",
        "            for i, w in links:\n",
        "                node_inputs.append(self.values[i] * w)\n",
        "            s = agg_func(node_inputs)\n",
        "            self.values[node] = act_func(bias + s)\n",
        "\n",
        "        return [self.values[i] for i in self.output_nodes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH5I8oKend0m"
      },
      "source": [
        "def train_single_genome_vcg(input_dataset, n_agents, model, filename=None, validation_set=None, n_epochs=5, init_lr=1e-4 , lrs_factor=0.1, lrs_patience=10, lrs_thresh=1e-4):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=init_lr)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=lrs_factor, patience=lrs_patience, threshold=lrs_thresh, threshold_mode='abs', verbose=True)\n",
        "\n",
        "    EPOCHS = n_epochs\n",
        "    marker = n_agents-1\n",
        "    num_samples = input_dataset.dataset.size()[0]\n",
        "    # state_dict_filename = filename\n",
        "    constraints_met_count = -INF\n",
        "    training_best_budget_constraint = INF\n",
        "    training_best_validation_bc = INF\n",
        "    training_best_wc_efficiency = -INF\n",
        "    training_best_validation_wc_efficiency = -INF\n",
        "    bc_hold_count = 0\n",
        "    validation_bc_hold_count = 0\n",
        "    training_best_epoch_max_loss = INF\n",
        "    training_stats = torch.zeros(4)\n",
        "    validation_stats = torch.zeros(4)\n",
        "    last_improvement_in_epoch = -1\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        budget_constraint_holds_epoch = True\n",
        "        efficiency_constraint_holds_epoch = True\n",
        "        epoch_max_budget_constraint = -INF\n",
        "        epoch_max_efficiency_constraint = -INF\n",
        "        epoch_worst_case_efficiency = INF\n",
        "        epoch_max_loss= -INF\n",
        "        sum_alpha=0\n",
        "\n",
        "        for batch in input_dataset:\n",
        "            batch_max_budget_constraint = -INF\n",
        "            batch_max_efficiency_constraint = -INF\n",
        "            batch_worst_case_efficiency = INF\n",
        "            for project in batch:\n",
        "                X = project\n",
        "                others_bids = torch.cat((X[:0], X[1:]), 0)\n",
        "                output = model.forward(others_bids.view(-1,marker))\n",
        "                for j, bid in enumerate(X):\n",
        "                    if j==0:\n",
        "                        continue\n",
        "                    others_bids = torch.cat((X[:j], X[j+1:]), 0)\n",
        "                    output_ = model.forward(others_bids.view(-1,marker))\n",
        "                    output = torch.cat((output, output_), 0)\n",
        "\n",
        "            budget_constraint, efficiency_constraint, efficiency_ratio = vcg_constraints(X[:n_agents], output)\n",
        "            sum_alpha+=efficiency_ratio\n",
        "            # Updating variables used in checkpointing\n",
        "            if budget_constraint > 0.0:\n",
        "                budget_constraint_holds_epoch = False\n",
        "            if efficiency_constraint > 0.0:\n",
        "                efficiency_constraint_holds_epoch = False\n",
        "            batch_max_budget_constraint = max(batch_max_budget_constraint, budget_constraint)\n",
        "            batch_max_efficiency_constraint = max(batch_max_efficiency_constraint, efficiency_constraint)\n",
        "            batch_worst_case_efficiency =  min(batch_worst_case_efficiency, efficiency_ratio)\n",
        "\n",
        "            epoch_max_budget_constraint = max(epoch_max_budget_constraint, batch_max_budget_constraint)\n",
        "            epoch_max_efficiency_constraint = max(epoch_max_efficiency_constraint, batch_max_efficiency_constraint)\n",
        "            epoch_worst_case_efficiency = min(epoch_worst_case_efficiency, batch_worst_case_efficiency)\n",
        "            loss = vcg_batch_loss(batch_max_budget_constraint, batch_max_efficiency_constraint, batch_worst_case_efficiency,  model.lambda_budget, model.lambda_efficiency)\n",
        "            epoch_max_loss = max(epoch_max_loss, loss)\n",
        "            # Backpropogating loss for each batch\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        epoch_mean_efficiency_ratio = sum_alpha/num_samples\n",
        "        training_best_epoch_max_loss = min(training_best_epoch_max_loss, epoch_max_loss)\n",
        "        epoch_data = torch.tensor([epoch_max_loss, epoch_max_budget_constraint, epoch_max_efficiency_constraint, epoch_worst_case_efficiency])\n",
        "        training_stats = torch.vstack((training_stats, epoch_data))\n",
        "        # Testing against validation set\n",
        "        # if validation_set is not None:\n",
        "        #     validation_result = model.eval(validation_set, n_agents, verbose=False, single_mode=False)\n",
        "        #     validation_wc_project, validation_bc_violating_project, validation_wc_bc, validation_wc_ec, validation_wc_efficiency, efficiency_ratios_class_func = validation_result\n",
        "        #     validation_loss = vcg_batch_loss(validation_wc_bc, validation_wc_ec, validation_wc_efficiency,  model.lambda_budget, model.lambda_efficiency, in_validation_mode=True)     \n",
        "        #     validation_data = torch.tensor([validation_loss, validation_wc_bc, validation_wc_efficiency, validation_wc_efficiency])\n",
        "        #     validation_stats = torch.vstack((validation_stats, validation_data))\n",
        "\n",
        "        # At the end of each epoch, take a checkpoint \n",
        "        if budget_constraint_holds_epoch:\n",
        "            if bc_hold_count<1 or (epoch_worst_case_efficiency > training_best_wc_efficiency and epoch_max_budget_constraint <= training_best_budget_constraint):\n",
        "                bc_hold_count+=1\n",
        "                # print('Moved closer to optimal worst-case performance, under budget constraint')\n",
        "                last_improvement_in_epoch=epoch\n",
        "                training_best_wc_efficiency = epoch_worst_case_efficiency\n",
        "                training_best_budget_constraint = epoch_max_budget_constraint\n",
        "                training_best_validation_bc = validation_wc_bc\n",
        "                training_best_validation_wc_efficiency = validation_wc_efficiency\n",
        "            # if validation_wc_bc<0 and validation_wc_efficiency>training_best_validation_wc_efficiency:\n",
        "            #     print('Saving checkpoint\\n')\n",
        "            #     save_checkpoint(state_dict_filename, model.state_dict())\n",
        "            # else:\n",
        "            #     print('No improvement in validation set. Not saving checkpoint\\n')\n",
        "\n",
        "\n",
        "        else:\n",
        "            if training_best_budget_constraint==INF and epoch_worst_case_efficiency > 0:\n",
        "                last_improvement_in_epoch=epoch\n",
        "                training_best_budget_constraint = epoch_max_budget_constraint\n",
        "                training_best_validation_bc = validation_wc_bc\n",
        "            elif epoch_max_budget_constraint < training_best_budget_constraint and (epoch_worst_case_efficiency > training_best_wc_efficiency):\n",
        "                # print('Budget constraint and worst-case efficiency improved!\\n')\n",
        "                last_improvement_in_epoch=epoch\n",
        "                training_best_budget_constraint = epoch_max_budget_constraint\n",
        "                training_best_wc_efficiency = epoch_worst_case_efficiency  \n",
        "                training_best_validation_bc = validation_wc_bc\n",
        "                training_best_validation_wc_efficiency = validation_wc_efficiency\n",
        "\n",
        "        # Update learning rate\n",
        "        if bc_hold_count>0:\n",
        "            lr_scheduler.step(-training_best_budget_constraint)\n",
        "                        \n",
        "        # print('Epoch ', epoch+1)\n",
        "        # # print('epoch_max_loss: ', epoch_max_loss)\n",
        "        # # print('training_best_epoch_max_loss: ', training_best_epoch_max_loss)\n",
        "        # print('bc_hold_count', bc_hold_count)\n",
        "        # print('last_improvement_in_epoch', last_improvement_in_epoch)\n",
        "        # print('training_best_budget_constraint: ', training_best_budget_constraint)\n",
        "        # print('training_best_wc_efficiency: ', training_best_wc_efficiency)\n",
        "        # print('epoch_max_budget_constraint: ', epoch_max_budget_constraint)\n",
        "        # print('epoch_worst_case_efficiency: ', epoch_worst_case_efficiency)\n",
        "        # if validation_set is not None:\n",
        "        #     print('>>>> training_best_validation_bc: ', training_best_validation_bc)\n",
        "        #     print('>>>> training_best_validation_wc_efficiency: ', training_best_validation_wc_efficiency)\n",
        "        #     print('>>>> validation_wc_budget_constraint: ', validation_wc_bc)\n",
        "        #     print('>>>> validation_wc_efficiency: ', validation_wc_efficiency)\n",
        "        #     print('-------------------------------------------')\n",
        "            # if validation_wc_bc<0 and epoch_max_budget_constraint<0 and epoch_worst_case_efficiency>=2/3 and validation_wc_efficiency>=2/3:\n",
        "            #   print('>>>> validation peak efficiency achieved_at: ', validation_wc_efficiency)\n",
        "            #   break\n",
        "\n",
        "    return model, epoch_max_budget_constraint, epoch_max_efficiency_constraint, epoch_worst_case_efficiency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjYIUhId7pY-"
      },
      "source": [
        "# link = (-1,3)\n",
        "w = 0.443\n",
        "weight = nn.Parameter(torch.tensor(w), requires_grad=True)\n",
        "print(float(weight.data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py6y6gUBhXdk"
      },
      "source": [
        "def train_genomes_vcg(genomes, config):\n",
        "    marker = n_agents-1\n",
        "    budget_surplus_allowance = 0.1\n",
        "    efficiency_surplus_allowance = 0.1\n",
        "    constraint_surplus = budget_surplus_allowance + budget_surplus_allowance\n",
        "    best_wc_budget_constraint = INF\n",
        "    best_wc_efficiency_constraint = INF\n",
        "    best_wc_efficiency = -INF\n",
        "    best_fitness = -INF\n",
        "    best_genome=None\n",
        "    trained_genomes = []\n",
        "\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(\"Mutation and reproduction complete, now training genomes\")\n",
        "    print(\"---------------------------------------------------------\\n\")\n",
        "\n",
        "    for genome_id, genome in genomes:\n",
        "        torchNet = FeedForwardTorchNet.create(genome, config)\n",
        "\n",
        "        # Measuring performance after training using SGD\n",
        "        trainedNet, trained_wc_bc, trained_wc_ec, trained_wc_alpha = train_single_genome_vcg(trainset, n_agents, torchNet)\n",
        "\n",
        "        # Update genome based on trained parameters    \n",
        "        for node_key, act_func, agg_func, bias, response, links in trainedNet.node_evals:\n",
        "            node_to_update = genome.nodes[node_key]\n",
        "            node_to_update.update('bias', bias)\n",
        "            for link in links:\n",
        "                connection_to_update = genome.connections[(link[0],node_key)]\n",
        "                connection_to_update.update('weight',float(link[1].data))\n",
        "\n",
        "        # Same fitness calculation as before\n",
        "        penalty1 = 100*trained_wc_bc if trained_wc_bc>0 else (1e-2)*trained_wc_bc\n",
        "        penalty2 = 10*trained_wc_ec if trained_wc_ec>0 else 0  \n",
        "        genome.fitness = wc_efficiency_reward_function(trained_wc_alpha, upper_bound) - penalty1 - penalty2\n",
        "\n",
        "        if genome.fitness>best_fitness:\n",
        "            best_fitness = genome.fitness\n",
        "            best_genome = genome\n",
        "            best_wc_budget_constraint = trained_wc_bc\n",
        "            best_wc_efficiency_constraint = trained_wc_ec\n",
        "            best_wc_efficiency = trained_wc_alpha\n",
        "\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(\"Post-training stats:\")    \n",
        "    print('>>> Best budget constraint: {0:3.5f}'.format(best_wc_budget_constraint))    \n",
        "    print('>>> Best efficiency constraint: {0:3.5f}'.format(best_wc_efficiency_constraint))    \n",
        "    print('>>> Best worst-case efficiency: {0:3.5f}'.format(best_wc_efficiency))\n",
        "    print(\"---------------------------------------------------------\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL4Y7S-OWv2e"
      },
      "source": [
        "\"\"\"Implements the core evolution algorithm.\"\"\"\n",
        "from neat.math_util import mean\n",
        "from neat.six_util import iteritems, itervalues\n",
        "\n",
        "\n",
        "class CompleteExtinctionException(Exception):\n",
        "    pass\n",
        "\n",
        "class Population(object):\n",
        "    \"\"\"\n",
        "    This class implements the core evolution algorithm:\n",
        "        1. Evaluate fitness of all genomes.\n",
        "        2. Check to see if the termination criterion is satisfied; exit if it is.\n",
        "        3. Generate the next generation from the current population.\n",
        "        4. Partition the new generation into species based on genetic similarity.\n",
        "        5. Go to 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, initial_state=None):\n",
        "        self.reporters = ReporterSet()\n",
        "        self.config = config\n",
        "        stagnation = config.stagnation_type(config.stagnation_config, self.reporters)\n",
        "        self.reproduction = config.reproduction_type(config.reproduction_config,\n",
        "                                                     self.reporters,\n",
        "                                                     stagnation)\n",
        "        if config.fitness_criterion == 'max':\n",
        "            self.fitness_criterion = max\n",
        "        elif config.fitness_criterion == 'min':\n",
        "            self.fitness_criterion = min\n",
        "        elif config.fitness_criterion == 'mean':\n",
        "            self.fitness_criterion = mean\n",
        "        elif not config.no_fitness_termination:\n",
        "            raise RuntimeError(\n",
        "                \"Unexpected fitness_criterion: {0!r}\".format(config.fitness_criterion))\n",
        "\n",
        "        if initial_state is None:\n",
        "            # Create a population from scratch, then partition into species.\n",
        "            self.population = self.reproduction.create_new(config.genome_type,\n",
        "                                                           config.genome_config,\n",
        "                                                           config.pop_size)\n",
        "            self.species = config.species_set_type(config.species_set_config, self.reporters)\n",
        "            self.generation = 0\n",
        "            self.species.speciate(config, self.population, self.generation)\n",
        "        else:\n",
        "            self.population, self.species, self.generation = initial_state\n",
        "\n",
        "        self.best_genome = None\n",
        "\n",
        "    def add_reporter(self, reporter):\n",
        "        self.reporters.add(reporter)\n",
        "\n",
        "    def remove_reporter(self, reporter):\n",
        "        self.reporters.remove(reporter)\n",
        "    \n",
        "    def run_sgd(self, epochs=5):\n",
        "        \"\"\"\n",
        "        Runs stochastic gradient-descent for a specified number of epochs \n",
        "        (default is 5).        \n",
        "        \"\"\"\n",
        "        \n",
        "        for epoch in epochs:\n",
        "            pass\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def run(self, fitness_function, training_function, n=None):\n",
        "        \"\"\"\n",
        "        Runs NEAT's genetic algorithm for at most n generations.  If n\n",
        "        is None, run until solution is found or extinction occurs.\n",
        "\n",
        "        The user-provided fitness_function must take only two arguments:\n",
        "            1. The population as a list of (genome id, genome) tuples.\n",
        "            2. The current configuration object.\n",
        "\n",
        "        The return value of the fitness function is ignored, but it must assign\n",
        "        a Python float to the `fitness` member of each genome.\n",
        "\n",
        "        The fitness function is free to maintain external state, perform\n",
        "        evaluations in parallel, etc.\n",
        "\n",
        "        It is assumed that fitness_function does not modify the list of genomes,\n",
        "        the genomes themselves (apart from updating the fitness member),\n",
        "        or the configuration object.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.config.no_fitness_termination and (n is None):\n",
        "            raise RuntimeError(\"Cannot have no generational limit with no fitness termination\")\n",
        "\n",
        "        k = 0\n",
        "        while n is None or k < n:\n",
        "            k += 1\n",
        "\n",
        "            self.reporters.start_generation(self.generation)\n",
        "\n",
        "            # Evaluate all genomes using the user-provided function.\n",
        "            fitness_function(list(iteritems(self.population)), self.config)\n",
        "\n",
        "            # Gather and report statistics.\n",
        "            best = None\n",
        "            for g in itervalues(self.population):\n",
        "                if best is None or g.fitness > best.fitness:\n",
        "                    best = g\n",
        "            self.reporters.post_evaluate(self.config, self.population, self.species, best)\n",
        "\n",
        "            # Track the best genome ever seen.\n",
        "            if self.best_genome is None or best.fitness > self.best_genome.fitness:\n",
        "                self.best_genome = best\n",
        "\n",
        "            if not self.config.no_fitness_termination:\n",
        "                # End if the fitness threshold is reached.\n",
        "                fv = self.fitness_criterion(g.fitness for g in itervalues(self.population))\n",
        "                if fv >= self.config.fitness_threshold:\n",
        "                    self.reporters.found_solution(self.config, self.generation, best)\n",
        "                    break\n",
        "\n",
        "            # Create the next generation from the current generation.\n",
        "            self.population = self.reproduction.reproduce(self.config, self.species,\n",
        "                                                          self.config.pop_size, self.generation)\n",
        "\n",
        "            # Check for complete extinction.\n",
        "            if not self.species.species:\n",
        "                self.reporters.complete_extinction()\n",
        "\n",
        "                # If requested by the user, create a completely new population,\n",
        "                # otherwise raise an exception.\n",
        "                if self.config.reset_on_extinction:\n",
        "                    self.population = self.reproduction.create_new(self.config.genome_type,\n",
        "                                                                   self.config.genome_config,\n",
        "                                                                   self.config.pop_size)\n",
        "                else:\n",
        "                    raise CompleteExtinctionException()\n",
        "\n",
        "            # Divide the new population into species.\n",
        "            self.species.speciate(self.config, self.population, self.generation)\n",
        "            upload_drive = create_mount_pydrive()\n",
        "\n",
        "            self.reporters.end_generation(self.config, self.population, self.species, upload_drive=upload_drive)\n",
        "\n",
        "            # Run SGD here, and update the population using the provided training function\n",
        "            training_function(list(iteritems(self.population)), self.config)\n",
        "\n",
        "            # Maybe run the reporters again after training?\n",
        "            # self.reporters.end_generation(self.config, self.population, self.species, upload_drive=upload_drive)\n",
        "\n",
        "            self.generation += 1\n",
        "\n",
        "        if self.config.no_fitness_termination:\n",
        "            self.reporters.found_solution(self.config, self.generation, self.best_genome)\n",
        "\n",
        "        return self.best_genome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSsCxMWhEPFc"
      },
      "source": [
        "test = [(1,3.0),(5,6.0)]\n",
        "test2 = [(w) for i,w in test]\n",
        "# print(torch.tensor(test2))\n",
        "param = nn.Parameter(torch.tensor(test2), requires_grad=True)\n",
        "def lelu_activation(z):\n",
        "    leaky = -0.0005\n",
        "    return z if z > 0.0 else leaky * z \n",
        "\n",
        "for j in param:\n",
        "    partial = (j*-2.0)\n",
        "    print(lelu_activation(partial))\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdqGrt0kHpck"
      },
      "source": [
        "import gzip\n",
        "import random\n",
        "import time\n",
        "\n",
        "try:\n",
        "    import cPickle as pickle  # pylint: disable=import-error\n",
        "except ImportError:\n",
        "    import pickle  # pylint: disable=import-error\n",
        "class Checkpointer(BaseReporter):\n",
        "    \"\"\"\n",
        "    A reporter class that performs checkpointing using `pickle`\n",
        "    to save and restore populations (and other aspects of the simulation state).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, generation_interval=100, time_interval_seconds=300,\n",
        "                 filename_prefix='neat-model2-checkpoint-'):\n",
        "        \"\"\"\n",
        "        Saves the current state (at the end of a generation) every ``generation_interval`` generations or\n",
        "        ``time_interval_seconds``, whichever happens first.\n",
        "\n",
        "        :param generation_interval: If not None, maximum number of generations between save intervals\n",
        "        :type generation_interval: int or None\n",
        "        :param time_interval_seconds: If not None, maximum number of seconds between checkpoint attempts\n",
        "        :type time_interval_seconds: float or None\n",
        "        :param str filename_prefix: Prefix for the filename (the end will be the generation number)\n",
        "        \"\"\"\n",
        "        self.generation_interval = generation_interval\n",
        "        self.time_interval_seconds = time_interval_seconds\n",
        "        self.filename_prefix = filename_prefix\n",
        "\n",
        "        self.current_generation = None\n",
        "        self.last_generation_checkpoint = -1\n",
        "        self.last_time_checkpoint = time.time()\n",
        "\n",
        "    def start_generation(self, generation):\n",
        "        self.current_generation = generation\n",
        "\n",
        "    def end_generation(self, config, population, species_set, upload_drive=None):\n",
        "        checkpoint_due = False\n",
        "\n",
        "        if self.time_interval_seconds is not None:\n",
        "            dt = time.time() - self.last_time_checkpoint\n",
        "            if dt >= self.time_interval_seconds:\n",
        "                checkpoint_due = True\n",
        "\n",
        "        if (checkpoint_due is False) and (self.generation_interval is not None):\n",
        "            dg = self.current_generation - self.last_generation_checkpoint\n",
        "            if dg >= self.generation_interval:\n",
        "                checkpoint_due = True\n",
        "\n",
        "        if (checkpoint_due) and (upload_drive is not None):\n",
        "            self.save_checkpoint(config, population, species_set, self.current_generation, upload_drive)\n",
        "            self.last_generation_checkpoint = self.current_generation\n",
        "            self.last_time_checkpoint = time.time()\n",
        "\n",
        "    def save_checkpoint(self, config, population, species_set, generation, upload_drive):\n",
        "        \"\"\" Save the current simulation state. \"\"\"\n",
        "        filename = '{0}{1}'.format(self.filename_prefix, generation)\n",
        "        print(\"Saving checkpoint to {0}\".format(filename))\n",
        "\n",
        "        with gzip.open(filename, 'w', compresslevel=5) as f:\n",
        "            data = (generation, config, population, species_set, random.getstate())\n",
        "            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        folder_id = '1NJ8abLpYebjVZscfuXmCdbrar8Gp67i2'\n",
        "        model_checkpoint = upload_drive.CreateFile({'parents':[{u'id': folder_id}]})\n",
        "        model_checkpoint.SetContentFile(filename)\n",
        "        model_checkpoint.Upload()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def restore_checkpoint(filename):\n",
        "        \"\"\"Resumes the simulation from a previous saved point.\"\"\"\n",
        "        with gzip.open(filename) as f:\n",
        "            generation, config, population, species_set, rndstate = pickle.load(f)\n",
        "            random.setstate(rndstate)\n",
        "            return Population(config, (population, species_set, generation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvEmGvF_oIsD"
      },
      "source": [
        "#  Loading or starting from scratch\n",
        "option_not_set=True\n",
        "reload_mode = False\n",
        "reload_filename = ''\n",
        "stats_filepath = ''\n",
        "init_lr = 1e-3\n",
        "while (option_not_set):\n",
        "    # clear_output(wait=True)\n",
        "    print(\"Please enter number to choose model reload mode:\\n\")\n",
        "    print(\"1. Reload existing model\")\n",
        "    print(\"2. Start from scratch\\n\")\n",
        "    option = input(\"option: \")\n",
        "    if (option=='1'):\n",
        "        option_not_set = False\n",
        "        reload_mode = True\n",
        "        init_lr = 1e-5\n",
        "        print(\"\\nPlease enter filename to reload model from:\")\n",
        "        reload_filename = input(\"filename: \")\n",
        "        print(f'Loading population from {reload_filename} ...')\n",
        "        # Loading trained model\n",
        "        drive.mount('/content/gdrive', force_remount=True)\n",
        "        GDRIVE_DIR = \"gdrive/My Drive/2. Adelaide Uni/Research Project/Evolutionary Models/Checkpoints\"\n",
        "        restored_population = Checkpointer.restore_checkpoint(os.path.join(GDRIVE_DIR, reload_filename))\n",
        "\n",
        "        print(\"Please enter filename for model stats\")        \n",
        "        stats_filename = input(\"filename for model stats: \")\n",
        "        stats_filepath = os.path.join(GDRIVE_DIR, \"Stats/\"+stats_filename)    \n",
        "        stats_reporter = StatisticsReporter(generation_interval=20, filename_prefix='neat-model2-stats-checkpoint-')\n",
        "        restore_checkpoint_result = stats_reporter.restore_checkpoint(stats_filepath)\n",
        "        if restore_checkpoint_result==0:\n",
        "            break\n",
        "        else:\n",
        "            print(restore_checkpoint_result)\n",
        "    elif (option=='2'):\n",
        "        option_not_set = False\n",
        "        print(\"Starting model from scratch!\\n\")\n",
        "        init_lr = 1e-4\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid input, try again!\\n\")\n",
        "        continue\n",
        "\n",
        "# neat-model2-stats-checkpoint-999\n",
        "# neat-model2-checkpoint-999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT0IQrcYdJCd"
      },
      "source": [
        "if (reload_mode):\n",
        "    for n, genome in enumerate(stats_reporter.most_fit_genomes):\n",
        "        print(f'Generation {n}:{genome.fitness}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiW3jkO1g1Lh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LyPUrsTcM-6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def lelu_activation(z):\n",
        "        leaky = -0.0005\n",
        "        return z if z > 0.0 else leaky * z    \n",
        "\n",
        "def run_vcg(config_file, n_agents, stats=None, restore_mode=False, restored_population=None):\n",
        "    marker = n_agents-1\n",
        "    # Load configuration.\n",
        "    \n",
        "    config = neat.Config(DefaultGenome, neat.DefaultReproduction,\n",
        "                         neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
        "                         config_file)\n",
        "\n",
        "    config.genome_config.activation_defs.add('lelu', lelu_activation)\n",
        "    # Create the population, which is the top-level object for a NEAT run.\n",
        "    if (restore_mode):\n",
        "        p = restored_population\n",
        "    else:\n",
        "        p = Population(config)\n",
        "\n",
        "    # Add a stdout reporter to show progress in the terminal.\n",
        "    p.add_reporter(StdOutReporter(True))\n",
        "    if (restore_mode):\n",
        "        p.add_reporter(stats)\n",
        "    else:\n",
        "        stats=StatisticsReporter(generation_interval=20, filename_prefix='neat-model2-stats-checkpoint-')\n",
        "        p.add_reporter(stats)\n",
        "    p.add_reporter(Checkpointer(generation_interval=20, time_interval_seconds=None, filename_prefix='neat-model2-checkpoint-'))\n",
        "\n",
        "    \n",
        "    # Run for specified number of generations.\n",
        "    # winner = p.run(eval_genomes_vcg, 1)\n",
        "\n",
        "    # Display the winning genome.\n",
        "    # print('\\nBest genome:\\n{!s}'.format(winner))\n",
        "\n",
        "    # for n, genome in enumerate(stats.most_fit_genomes):\n",
        "    #     print(f'Generation {n}:{genome.fitness}')\n",
        "\n",
        "    return p\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBem_Am6PBUK",
        "scrolled": false
      },
      "source": [
        "# Loading config file\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "GDRIVE_DIR = \"gdrive/My Drive/2. Adelaide Uni/Research Project/Evolutionary Models/Imports\"\n",
        "config_filename = \"public-project-config-two.py\"\n",
        "session_config_filename = os.path.join(GDRIVE_DIR, config_filename)\n",
        "\n",
        "if (reload_mode):\n",
        "    population = run_vcg(session_config_filename, n_agents, stats_reporter, restore_mode=True, restored_population=restored_population)\n",
        "else:\n",
        "    population = run_vcg(session_config_filename, n_agents)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7dggJi-BfqG"
      },
      "source": [
        "config = neat.Config(DefaultGenome, neat.DefaultReproduction,\n",
        "                         neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
        "                         session_config_filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VORtD7IWNzXF"
      },
      "source": [
        "print(config.genome_config.input_keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaRUHYBzASvG"
      },
      "source": [
        "print(config.genome_config.output_keys + config.genome_config.input_keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGJLepmP_1Up"
      },
      "source": [
        "values = dict((key, 0.0) for key in config.genome_config.output_keys + config.genome_config.input_keys)\n",
        "print(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZKYXw7yzAyY"
      },
      "source": [
        "# for v in list():\n",
        "def lelu_activation(z):\n",
        "    leaky = -0.0005\n",
        "    return z if z > 0.0 else leaky * z\n",
        "\n",
        "config.genome_config.activation_defs.add('lelu', lelu_activation)\n",
        "print(config.genome_config.activation_defs.get('lelu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r29mHDtYVgRF"
      },
      "source": [
        "print(len(population.population.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPZNq0x3s6h6"
      },
      "source": [
        "for genome in population.population.values():\n",
        "\n",
        "    for key, node in genome.nodes.items():\n",
        "        node.update('bias',)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    for connection in genome.connections.values():\n",
        "        print(connection)\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3KngyJtuQoa"
      },
      "source": [
        "\n",
        "\n",
        "for genome in population.population.values():\n",
        "    # print(genome)\n",
        "    # net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
        "    # print(net)\n",
        "    for node in genome.nodes.values():\n",
        "        print(node)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    for connection in genome.connections.values():\n",
        "        print(connection.key)\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_1EYT5j49fS"
      },
      "source": [
        "# best_genome = evolution_stats.best_genome()\n",
        "# marker  = n_agents-1\n",
        "# config = neat.Config(DefaultGenome, neat.DefaultReproduction,\n",
        "#                          neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
        "#                          session_config_filename)\n",
        "# best_net = neat.nn.FeedForwardNetwork.create(best_genome, config)\n",
        "# max_budget_constraint, max_efficiency_constraint, worst_case_efficiency = eval_single_genome(best_net, marker)\n",
        "# penalty1 = 100*max_budget_constraint if max_budget_constraint>0 else 0 \n",
        "# penalty2 = 100*max_efficiency_constraint if max_efficiency_constraint>0 else 0  \n",
        "# best_fitness = min(worst_case_efficiency, upper_bound) - penalty1 - penalty2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hzwVgLzDp3n"
      },
      "source": [
        "# print('max_budget_constraint: ', max_budget_constraint)\n",
        "# print('max_efficiency_constraint: ', max_efficiency_constraint)\n",
        "# print('worst_case_efficiency: ', worst_case_efficiency)\n",
        "# print('best_fitness: ', best_fitness)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}